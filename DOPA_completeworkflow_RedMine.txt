DOPA complete workflow
======================

November 2019 - Documentation of the complete worklow

# INPUT DATASETS

To be documented:

* source (website? request? script?)
* notes (data specific)
* preprocessing (geometric cleanup, transformation, etc)
* scripts location

## BASE LAYERS

### WDPA

After downloading of the WDPA zip file, a series of steps are run in order to pre-process and prepare all the necessary tables and layers for the subsequent analysis phase. All the scripts needed for this phase are stored into **/globes/current\_processing/wdpa\_preprocessing/** folder.

**NOTES**
All parameters used by scripts are stored in the configuration file **/globes/current\_processing/service\_files/wdpa\_preprocessing.conf**. It has to be checked/edited before running the scripts.

N.B. passwords for connection to PostgreSQL database are not explicitly written  in the .conf file. Instead, they are read from the docker .pgpass file. Different settings on different docker machines could have different beahvours and it could be needed to edit the definition of the 'dbpar1' and/or 'dbpar2' in the exec* scripts.

Steps from 1 to 3 are devoted to the preparation of the three base wdpa datasets: all PAs, PAs over 5 km2 and 10km buffers on PAs over 10 km2.

Steps from 4 to 8 are devoted to transfer of  PAs over 10 km2 and of buffers in GRASS for subsequent analysis.


#### **1. SOURCE**

#### **2. IMPORT**

  2.1 Imports in PG database relevant WDPA data: access the downloaded .zip file, imports attributes and geometries of points and polygons in three distinct tables of PG database.
   Objects are filtered as follows:
	
    - points: `STATUS NOT IN ('Not Reported', 'Proposed') And DESIG_ENG NOT IN ('UNESCO-MAB Biosphere Reserve') And REP_AREA > 0`
	
    - polygons: `STATUS NOT IN ('Not Reported', 'Proposed') And DESIG_ENG NOT IN ('UNESCO-MAB Biosphere Reserve')  And WDPAID NOT IN (903141)`
	
  N.B.: from February 2018: wdpaid 903141 (Primeval Beech Forests of the Carpathians and Other Regions of Europe) is excluded from all the analysis.
	
  2.2 Buffering of point PAs. Buffer radius is computed from the 'rep_area' field.

  2.3 Flagging invalid geometries and repairing them with ST_MakeValid.

  2.4 Creation of final wdpa table with attributes and wdpa_o5 (>5 sq.km.) table.

**SCRIPTS** (to be executed in this order): 

`exec_wdpa_preprocessing_part_1.sh` and its slave `wdpa_preprocessing_part_1.sql`

`fix_wdpa_geom.sql` , to be executed in postgis **STEP BY STEP**, checking results after each step and, in case of need, manually repairing invalid geometries.

`exec_wdpa_preprocessing_part_2.sh` and its slave `wdpa_preprocessing_part_2.sql`

#### **3. Creation of 10 km buffers**

  3.1 Computes buffers on all features.

  3.2 Selects buffers intersecting anti-meridian, shift corresponding PAs, recompute buffers and shift them back.

  3.3 Creates the final buffers table.

**SCRIPTS**: `exec_buffers_processing.sh` and its slave `wdpa_buffers_processing.sql`; 

**N.B.** The 10km buffers computed here on PAs over 5 km2 are 'gross' buffers, while the 'unprotected' buffers are computed in GRASS (see Step 6 below).


#### **4. Data preparation for GRASS**
 
  4.1 Creates necessary schemes for PAs and buffers.

  4.2 Creates PAs and buffers lists (full list, only terrestrial and coastal, only marine) as views and as text files, for later use in GRASS analysis loops.

  4.3 Creates individual views for each PA and for each Buffer.

**NOTES**
Text files with lists of PAs and buffers are used in all the GRASS based scripts for data analysis (see relevant section).

**SCRIPTS**: `exec_prepare_data_x_grass.sh` and its slave `prepare_data_x_grass.sql`

#### **5. Creation of wdpa_flat** (required to produce at a later stage unprotected buffers in GRASS)
This flat is required in order to compute unprotected buffers (see Step 6)

  5.1 Import in GRASS database the whole WDPA dataset, without attributes table.

  5.2 Running in parallel on 32 different regions, converts to raster the vector WDPA at 3 arc-seconds resolution (about 90m at the equator).

  5.3 Using r.patch, mosaic the 32 raster tiles into a single raster layer.

**SCRIPTS**:`exec_make_flat.sh` and its slave `slave_make_flat.sh`

#### **6. Data transfer from PG to GRASS**

  6.1 Iterates views of PAs and buffers to export each view in shapefile with pgsql2shp.

  6.2	Imports individual shapefiles of PAs in GRASS database using v.in.ogr.

  6.3 Import and process individual shapefiles of BUs: buffers are erased with wdpa_flat in order to keep only their unprotected portion.

**SCRIPTS**:`exec_pg_to_grass.sh` and its slaves `slave_pg_to_grass.sh` and `slave_unprot_buffer.sh`

**NOTES**
Direct import in GRASS of PG views with v.in.ogr randomly fails on about 20% of views for unknown reasons (error message: Segmentation fault). This is why we moved to export/import shp.

**N.B.** Recommended n. of cores for this step: **no more than 40 cores (edit wdpa_preprocessing.conf accordingly)**. Using more than 40 cores results in FATAL errors of pgsql2shp (too many connections).

#### **7. Reprojection in Mollweide of individual PAs and BUs layers** 

  6.1 Iterates vector layers in GRASS database (WGS84LL location) of PAs and buffers and reproject them in Mollweide. Results are stored into MOLLWEIDE location.

**SCRIPTS**:`exec_project_pa_bu.sh` and its slave `slave_project_pa_bu.sh`

#### **8. Raster data import in GRASS Database** 

  7.1 Import as links (using r.external) to external Tiff or vrt files all the raster dataset needed for calculation of indicators. 

The GRASS Database to be used for import is defined by the configuration file  **/globes/current\_processing/service\_files/wdpa\_preprocessing.conf**.

**N.B. This script needs to be run only if a new GRASS database is used or if some raster datasets are changed.**

The following datasets are physically imported (with r.import) instead of linked because of problems with management of No Data values in the Tiff file.


* Worldclim (tavg, tmax and tmin)

* Global Livestock of the world v.3

Also, Sea Surface Temperature data (distributed as NETCDF file, too complex to be managed with r.external) are physically imported with r.in.gdal.

Most of the raster datasets are imported 'as they are' from the /spatial\_data/Original\_Datasets folder but there are some exceptions:

* Global Forest Change data need to be preprocessed to mask pixels with treecover value<30. Also, gain and lossyear datasets need to be reprojected in Mollweide

* Groads is a raster dataset obtained through processing of the Groads vector layer. 

* Global Surface Water, ESA-CCI Land cover, Morphological Spatial Pattern Analysis, GHSL Built Up and Landscape Productivity Dynamics  data need to be reprojected in Mollweide.

* Raster files of the THDI package are not imported by the script. They are included in the GRASS database /globes/USERS/GIACOMO/eHabitat/hdi_db/global_MW/ehabitat/ (see more details in the 'WDPA INDICATORS: GRASS APPROACH' section below).

Processed/modified raster datasets are stored in /spatial\_data/Derived\_datasets/ 

Scripts to pre-process each of the above mentioned derived rasters are stored in /globes/processing\_current/raster\_processing/ 

**SCRIPTS**:`exec_link_rasters.sh` and its slaves `slave_link_rasters_wgs84ll.sh`, `slave_link_rasters_mollweide.sh` and `slave_link_rasters_sst.sh`

If new raster datasets need to be added, the slave scripts will have to be edited accordingly, using the following rules:

* Continuous raster in Lat/Long: edit script `slave_link_rasters_wgs84ll.sh` and import it into CONRASTERS mapset
* Categorical raster in Lat/Long: edit script `slave_link_rasters_wgs84ll.sh` and import it into CATRASTERS mapset
* Continuous raster in MOLLWEIDE: edit script `slave_link_rasters_mollweide.sh` and import it into CONRASTERS mapset
* Categorical raster in MOLLWEIDE: edit script `slave_link_rasters_mollweide.sh` and import it into CATRASTERS mapset.

### COUNTRIES (2019)

#### **Sources**

*  **ISO CODES**. ISO 3166 officially assigned country codes from International Organization for Standardization, Geneva. Available online at https://www.iso.org/obp/ui/#search. Downloaded on 20191122.

*  **EEZ V11**. Flanders Marine Institute (2019). Maritime Boundaries Geodatabase, version 11. Available online at http://www.marineregions.org/. https://doi.org/10.14284/382. Downloaded on 20191122.

Original dataset are kept in /SPATIAL\_DATA/ORIGINAL\_DATASETS/ folder, within EEZ, ISO\_3166 subfolders.

#### **Preprocessing**

A flat topological corrected layer for EEZ has been obtained in PostGIS, using the script http://h05-redmine.jrc.it/attachments/download/315/eez_2019.sql, also available in /GLOBES/processing\_current/eez_2019.sql.

The final version is included as geopackage in **/spatial\_data/Derived\_Datasets/eez\_2019.gpkg**.

### ECOREGIONS (V2019)

#### **Sources**

*  Olson, D. M., Dinerstein, E., Wikramanayake, E. D., Burgess, N. D., Powell, G. V. N., Underwood, E. C., D'Amico, J. A., Itoua, I., Strand, H. E., Morrison, J. C., Loucks, C. J., Allnutt, T. F., Ricketts, T. H., Kura, Y., Lamoreux, J. F., Wettengel, W. W., Hedao, P., Kassem, K. R. 2001. Terrestrial ecoregions of the world: a new map of life on Earth. Bioscience 51(11):933-938. Downloaded on 201603 from http://www.worldwildlife.org/publications/terrestrial-ecoregions-of-the-world.

*  The Nature Conservancy (2012). Marine Ecoregions and Pelagic Provinces of the World. GIS layers developed by The Nature Conservancy with multiple partners, combined from Spalding et al. (2007) and Spalding et al. (2012). Cambridge (UK): The Nature Conservancy. Downloaded on 20160720 from http://data.unep-wcmc.org/datasets/38. Version with complete coastline has been used. **This is different from previous version, where NoCoast Version has been used. This is due to the better coastline of MEOW/PPOW (eg: a stripe of about 1000x1 Km was missing from Adriatic land), and to the fact that "Saint Pierre et Michelon" and "Tokealu"  islands were obliterated by the NoCoast version.**

Original dataset are kept in /SPATIAL\_DATA/ORIGINAL\_DATASETS/ folder, within TEOW and WCMC/MEOW\_PPOW/ subfolders

#### **Preprocessing**

A flat topological corrected layer, integrating the 3 sources [teow+(meow+ppow)] has been obtained:

1. assigning unique numeric id to each class in the original layers:
  1.  for TEOW the original ECO\_ID field has been used. **The REALM info "AT" (Afrotropics) and "NT" (Neotropics) for ECO\_IDs -9999 (Rock and Ice) and -9998 (Lake) is not included because, since for polar regions is NULL, it produces redundancy on the ECO_ID field (primary key in the final dataset)**.
  2.  for MEOW and PPOW, for historical reasons, the same IDs previously assigned by JRC (reviewed with Bastian Bertzky in 2015) has been used, with a JOIN based on multiple fields (ECOREGION,PROVINC,BIOME or REALM, depending from the source), adapting original names when needed.
  3.  correspondence table within original fields and final attributes is saved in the final geopackage as "lookup\_attribute\_table", where the boolean fields "eco/pro/bio\_is\_mod" identify rows where ECOREGION/PROVINC/BIOME content has been modified (respect to "ORIGINAL\_*" fields, included) to match the names in the final attributes (first\_level, second\_level, third\_level), allowing this way the join.
     *  first\_level corresponds to TEOW Ecoregion, MEOW Ecoregion, PPOW Province
     *  second\_level corresponds to TEOW Biome, MEOW Province, PPOW Biome
     *  third\_level corresponds to REALM for TEOW, MEOW and PPOW.
2. overlapping (ArcGIS PRO UNION) MEOW_PPOW with a Bounding Box Polygon (±180/90) as EEOW source (Empty Ecoregions of the World!), with the code 100001-"unassigned land ecoregion"
3. assigning MEOW/PPOW polygons intersecting EEOW to MEOW/PPOW. EEOW only polygons are left untouched
4. overlapping (ArcGIS PRO UNION) the TEOW+MEOW\_PPOW\_EEOW from the previous step
5. assigning:
  1. TEOW, MEOW or PPOW only polygons to TEOW, MEOW or PPOW
  2. TEOW polygons intersecting EEOW to TEOW
  3. TEOW polygons intersecting MEOW or PPOW to MEOW or PPOW. **This is different from previous version, where TEOW overlapped MEOW/PPOW. This is due to the better MEOW/PPOW coastline.**
     *  **exception to the above: 5 polygons intersecting both meow/ppow (codes ppow-9 and meow-20073,20077) and teow have been assigned to teow, because they were the only polygons assigned to teow  61318-"St. Peter and St. Paul rocks" and 60172-"Trindade-Martin Vaz Islands tropical forests".** 
  5. To identify features not reaching ±180/90, EEOW only have been exploded to singlepart, then intersected with a multiline created at -0.5 arcsec (about 15 meters at equator) from extremes. This way:
    1. an unassigned stripe of 360dx15 Km has been flagged as real antarctic land (teow),
    2. few polygons (11 originally) have been manually split (20 polygons), then:
       *  the 14 parts adjoining TEOW and extremes have been assigned to the correspondent TEOW classes
       *  the other 6 parts have been left unchanged (EEOW).

The result of the above is included in the final geopackage as **ecoregions\_2019\_raw** spatial layer as undissolved, single part polygons, where for each of them is reported:

*  source (teow, meow, ppow, eeow)
*  eco\_id (first\_level\_code)
*  notes:
   *  "originally teow/meow/ppow/eeow" = attribute unchanged
   *  "assigned to meow/ppow" = originally teow, assigned to meow/ppow
   *  "assigned to teow" = originally  meow/ppow, assigned to teow (codes ppow-9 and meow-20073,20077; assigned to teow  61318-"St. Peter and St. Paul rocks" and 60172-"Trindade-Martin Vaz Islands tropical forests")
   *  "reassigned to teow" = originally eeow, assigned to adjacent teow classes (eg: antarctic land adjoining south pole), some with with manual split (14 parts crossing ±180)
   *  "reassigned to eeow" = originally eeow, split in the previous step.

Above object has been dissolved in ArcGIS PRO (returing the expect 1097 classes) then exploded as single part polygons.  **NB: any correction to ecoregions should be applied to teow\_meow\_ppow\_eeow_raw, then dissolve it again to get the final version.**

This version has been imported in PostGIS, then checked for geometry validity, fixed, finalized (single and multiparts) with the script http://h05-redmine.jrc.it/attachments/download/314/ecoregions_2019.sql, also available in /GLOBES/processing\_current/ecoregions\_2019.sql. The same script contains also method to calculate statistics (source and ecoregions change), as discussed with Luca Battistella, also available as http://h05-redmine.jrc.it/attachments/download/313/ecoregions_2019_statistics.xlsx, and http://h05-redmine.jrc.it/attachments/download/312/ecoregions_2019.sql, also available in /GLOBES/processing\_current/ecoregions\_2019\_statistics.xlsx.

The final version is included as geopackage in **/spatial\_data/Derived\_Datasets/ecoregions\_2019.gpkg**, wich includes:

*  ecoregions\_2019 multipart
*  ecoregions\_2019\_raw (undissolved, single part polygons)
*  lookup\_attribute\_table (correspondence table within original fields and final attributes).

## THEMATIC LAYERS

### SPECIES

# PROCESSING

## CEP layer

This layer is created building a pseudo-topology merging information from countries, ecoregions and protected areas.
Pseudo-topology is meant as a simple feature, global layer, flattened, which keeps the information from the original sources at the level of each single atomic geometry in which it can be de-structured.
Building a real topology would be too expensive due to the size of the input datasets.

Key parts of the process are:

*  tiling: world is split in tiles of one degree. Each process can be parallelized distributing a certain number of tiles to each core
*  pseudo-rasterization: in every tile, every object is temporary rasterized at 1 arcsec (about 30 meters at equator), then vectorized. This step, despite increasing the number of vector nodes participating to each tile, simplify the overlapping geometries, in a more efficient way of other approaches that have been tested (snap to grid; snap to other geometries within the tile; simplify; the already mentioned, unsustainable, topology-building).

### INFRASTRUCTURE

Everything happens within the postgres schema **cep\_processing**, but the `workflow_parameters.conf` document allows setup of the system through variables (eg: `DB` allows to change database, `SCH` allows to change schema, etc...) 

The script `01_create_infrastructure.sh` builds all the needed functions and tables, and should be executed only if starting from scratch.

### INPUT DATASETS

scripts:

*  `02_input_wdpa`, launched as `./02_input_country.sh 72 > logs/02_input_country_log.txt 2>&1`
*  `02_input_ecoregion`, launched as `./02_input_ecoregions.sh 72 > logs/02_input_ecoregion_log.txt 2>&1`
*  `02_input_country`, launched as `./02_input_wdpa.sh 72 > logs/02_input_wdpa_log.txt  2>&1`

through temporary functions (destroyed at the end of the run), import current dataset inside the tables:

*  `input_wdpa`
*  `input_ecoregion`
*  `input_country`

Above tables contain temporary unique numeric id (tid), geom (Polygon, single part, 4326), and original id from the sources (wdpaid, first\_level\_code, country\_id **aliased as fid**). 

Variables:

*  `WDPA_VERSION="wdpa_201911"`
*  `PAFID="wdpaid"`
*  `ECOREGION_VERSION="ecoregions_2019"`
*  `ECOFID="first_level_code"`
*  `COUNTRY_VERSION="gaul_eez_dissolved"`
*  `COUNTRYFID="country_id"`

in the `workflow_parameters.conf` document, allow the control of the dataset version to be imported, and the original id to be aliased as fid.

**NB:**

*  import of countries gave one invalid geometry, which has been manually fixed with the script `fix_country.sql`. The validity report fields have not been updated, just to keep track of the wrong geometry.
*  import of wdpa gave one invalid geometries, which has been manually fixed with the script `fix_wdpa.sql`. Since this is a patch to the data already processed by Giacomo, which do not have validity report fields for this version, this is not tracked. I have asked Giacomo to include the validity report fields in future releases.

### TILING

script:

`./03_clip.sh`, launched as `./03_clip.sh 72 > logs/03_clip_log.txt 2>&1`

The tiling task involves at the same time the 3 input dataset (country, ecoregion, wdpa \_input).
The script will build a table (clip\_onprocess; destroyed at the end of the run) to count the input objects (tiles in the grid), in the way to spread through the available threads (designated by the second number in the launching script: 72, in the example) subset of tiles, on which applying the following operations. 

For each tile (identified by its primary key: qid) in the subset, the same clipping function (f\_clip) will be applied (in parallel) to each input dataset (in sequence).
The result is written to the 3 tables:

*  country\_clip
*  ecoregion\_clip
*  wdpa\_clip

The f\_clip function and the 3 country, ecoregion, wdpa \_clip tables are prepared as infrastructure.
The script **will truncate** existing rows in the output tables at launch.
A check of the geometry is applied at the end of all the loops.

### RASTER

script:

`./04_rast.sh`, launched as `./04_rast.sh 72 > logs/04_rast_log.txt 2>&1`

The rasterizing task involves at the same time the 3 input dataset (country, ecoregion, wdpa \_clip).

The script will build a table (rast\_onprocess; destroyed at the end of the run) to count the input objects (tiles in the grid), in the way to spread through the available threads (designated by the second number in the launching script: 72, in the example) subset of tiles, on which applying the following operations. 

For each tile (identified by its primary key: qid) in the subset, the same rasterazing function (f\_raster) will be applied (in parallel) to each input dataset (in sequence).

The f\_raster function will loop through the** tid (temporary id)** of the input dataset, intersecting the input polygon with an empty raster (1 arc sec resolution; about 30 meters at equator), aggregating the resulting pixels by the **fid (country, ecoregion or wdpa original unique id)**.

The result is written to the 3 tables:

*  country\_rast
*  ecoregion\_rast
*  wdpa\_rast

The f\_clip function and the 3 country, ecoregion, wdpa \_rast tables are prepared as infrastructure.
The script **will truncate** existing rows in the output tables at launch.


### PSEUDO-RASTER

### FLAT

## WDPA INDICATORS: GRASS APPROACH

### COMPUTING INDICATORS FOR PAs AND 10km BUFFERS

With the GRASS approach, indicators are computed with two basic tools, depending on the type of raster dataset. 
Categorical rasters are analyzed using r.stats to compute, for each PA and, when needed, for each unprotected buffer (hereinafter BU) , the areas of each class.
Continuous rasters are analyzed using r.univar to compute, for each PA and, when needed, each BU, basic statistics (min, max mean, etc.)

Once the pre-processing of WDPA is completed (see Base Layers/WDPA above) the set of scripts described below allows the computation of most of the indicators populating DOPA Explorer.
Each script runs in parallel on the user-specified number of cores, using GNU parallel, by means of:

* dynamic subdivision of PAs or BUs list (previously created) into n sub-lists (where n is the user defined number of cores to be used)
* creation of n temporary mapsets
* run in parallel of dynamically built scripts inside each temporary mapset.
* merging of results for each mapset

N. of cores, paths for log files, results and other variables needed by the scripts are defined by the configuration file **/globes/current\_processing/service\_files/wdpa\_processing.conf**. It has to be checked/edited before running the scripts.

For those raster having the same spatial resolution and extent, a single script is used to analyze them, in order to optimize processing time.

Depending on the raster type (continuous or categorical), each script works in the WGS84LL location or the MOLLWEIDE location (see variables 'LOCATION\_LL\_PATH' or 'LOCATION\_MO\_PATH' in the 'Derived variables' section of each script.

The following table provides the full list of raster datasets processed:

**/globes/processing\_current/servicefiles/dataset\_list\_2020.htm**

For each of them, the following info is provided: 

* corresponding indicator name
* full path to raster files
* layer name in GRASS database
* name(s) of script(s) used for analysis

All the scripts are stored into **/globes/current\_processing/wdpa\_processing/** folder.

**NOTES**

1) Only 'exec\_*' scripts need to be run. All the other scripts are slaves, called by the corresponding 'exec\_' script.

2) The script for Terrestrial Habitat Diversity Indicator work only if it is run:

* from a dedicated docker container with the followings installed:
<ul> 
* GRASS 7.1 
</ul>
<ul> 
* Python 2.7 with numpy, os, csv, sys, tqdm libraries
</ul>

   Presently there is an _ad hoc_ docker container (name: 'jdelligi') installed on dopaprc server. Contact IT to get a clone.

* using a GRASS database that contains all the raster layers included in the mapset 'rasterized\_parks' of the GRASS database globes/USERS/GIACOMO/eHabitat/hdi\_db/ (location : global\_MW) and a group file named 'segm' in the mapset 'ehabitat', containing the list of rasters to be analyzed. The same rasters have been stored as TIF files into **/spatial\_data/Derived\_Datasets/RASTER/THDI/** folder.

N.B. The slave script contains several hard-coded variables: check it carefully before run.

3) Scripts for computation of indicators for buffers are provided only for pressures. For this reason, Copernicus land cover analyzed also for buffers because class 40 (cropland) % is used as metric for Agricultural pressure in PAs and buffers.

### POST-PROCESSING PAs AND 10km BUFFERS INDICATORS DATA
In this step data computed in GRASS during the previous step are imported and processed in PostgreSQL database in a series of tables ready for subsequent aggregation.

The PostgreSQL database used by scripts is **wolfe**, hosted on **s-jrciprap247p.ies.jrc.it** server.

The following table lists the WDPA indicators, the corresponding script used to post-process data previously computed in GRASS and the corresponding schema and table name in PostgreSQL database:

**/globes/processing\_current/servicefiles/postprocessing\_scripts\_2020.htm**  (IN PREPARATION)



[...]

### WDPA DATA AGGREGATION
In this step all PAs and 10km buffers indicators are collected into a single table (**wdpa\_all\_inds**) feeding up the REST service(s) for DOPA Explorer. There are a few exceptions, i.e. data kept in separate tables and read by separate REST services.

[...]



## COUNTRIES / ECOREGIONS / WDPA INDICATORS: CEP APPROACH

[...]